{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f65bc5-f4ad-4a20-a4c5-03f03b3ce246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Replace these paths with your actual directory paths\n",
    "source_directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_location-main\\\\dataset_location-main'\n",
    "destination_directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_directory_path, exist_ok=True)\n",
    "\n",
    "# Walk through the source directory\n",
    "for root, dirs, files in os.walk(source_directory_path):\n",
    "    # Check if we're inside the Speed_data directory, but not directly in it\n",
    "    if 'Speed_data' in root and root.strip('\\\\').endswith('Speed_data'):\n",
    "        # Only process files in Speed_data, do not go into its subdirectories\n",
    "        dirs[:] = []  # This clears the dirs list in-place, preventing os.walk from going into any subdirectories\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Construct the original full path of the file\n",
    "            original_file_path = os.path.join(root, file)\n",
    "            # Extract the subdirectory name\n",
    "            subdirectory_name = os.path.basename(os.path.normpath(root))\n",
    "            # Construct the new file name and path\n",
    "            new_file_name = f\"{subdirectory_name}_{file}\"\n",
    "            destination_file_path = os.path.join(destination_directory_path, new_file_name)\n",
    "            # Copy the file\n",
    "            shutil.copy(original_file_path, destination_file_path)\n",
    "\n",
    "print(\"CSV files have been copied and renamed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b72384-dbed-4ceb-ba1b-8417396ac7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this path with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Check if the number of columns is not exactly 17\n",
    "            if df.shape[1] != 17:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b13d7-c195-4aab-b5b4-b3641085cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Replace these with your desired column names\n",
    "column_names = ['deviceID', 'date', 'longitude', 'latitude', 'altitude', 'accelerometer x'\n",
    "                , 'accelerometer y', 'accelerometer z', 'userAccelerometer x', 'userAccelerometer y'\n",
    "                , 'userAccelerometer z', 'gyroscope x', 'gyroscope y', 'gyroscope z', 'magnetometer x', 'magnetometer y', 'magnetometer z'\n",
    "]\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            # Read the CSV file without header\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            # Assign the column names\n",
    "            df.columns = column_names\n",
    "            # Save the file back\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Processed: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57587479-8f9d-4230-b1cb-1719f342df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Names of the latitude and longitude columns\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "\n",
    "# Check if a value is within the latitude range\n",
    "def is_valid_latitude(lat):\n",
    "    return -90 <= lat <= 90\n",
    "\n",
    "# Check if a value is within the longitude range\n",
    "def is_valid_longitude(lon):\n",
    "    return -180 <= lon <= 180\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if latitude and longitude columns exist\n",
    "            if latitude_column_name in df.columns and longitude_column_name in df.columns:\n",
    "                # Apply validity checks\n",
    "                invalid_latitudes = df[~df[latitude_column_name].apply(is_valid_latitude)]\n",
    "                invalid_longitudes = df[~df[longitude_column_name].apply(is_valid_longitude)]\n",
    "                \n",
    "                # Report findings\n",
    "                if not invalid_latitudes.empty or not invalid_longitudes.empty:\n",
    "                    print(f\"File '{file_name}' contains invalid latitude or longitude values:\")\n",
    "                    if not invalid_latitudes.empty:\n",
    "                        print(f\"Invalid latitudes:\\n{invalid_latitudes[[latitude_column_name]]}\")\n",
    "                    if not invalid_longitudes.empty:\n",
    "                        print(f\"Invalid longitudes:\\n{invalid_longitudes[[longitude_column_name]]}\")\n",
    "                else:\n",
    "                    print(f\"File '{file_name}' has all valid latitude and longitude values.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' does not contain the specified latitude and longitude columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44baa1ed-b71e-4717-83d7-e00d56393a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Names of the latitude and longitude columns\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "\n",
    "# Check if a value is within the latitude range\n",
    "def is_valid_latitude(lat):\n",
    "    return -90 <= lat <= 90\n",
    "\n",
    "# Check if a value is within the longitude range\n",
    "def is_valid_longitude(lon):\n",
    "    return -180 <= lon <= 180\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if latitude and longitude columns exist\n",
    "            if latitude_column_name in df.columns and longitude_column_name in df.columns:\n",
    "                # Apply validity checks\n",
    "                invalid_latitudes = df[~df[latitude_column_name].apply(is_valid_latitude)]\n",
    "                invalid_longitudes = df[~df[longitude_column_name].apply(is_valid_longitude)]\n",
    "                \n",
    "                # If there are any invalid values, delete the file\n",
    "                if not invalid_latitudes.empty or not invalid_longitudes.empty:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted '{file_name}' due to invalid latitude or longitude values.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' does not contain the specified latitude and longitude columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete. Invalid files have been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ab090-dbe5-4c2f-858c-a800bc163265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# List the names of the columns you want to keep\n",
    "important_columns = ['date', 'longitude', 'latitude']\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Keep only the important columns, if they exist in the dataframe\n",
    "            df_filtered = df[important_columns] if all(col in df for col in important_columns) else df\n",
    "            \n",
    "            # Save the filtered dataframe back to the same file, overwriting the original\n",
    "            df_filtered.to_csv(file_path, index=False)\n",
    "            print(f\"Processed and saved: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete. Files have been overwritten with only important columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78324552-f0db-4f62-84f9-08dbd79f7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Names of the latitude and longitude columns\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "\n",
    "# Function to check for NaN values or zeros\n",
    "def has_invalid_values(series):\n",
    "    return series.isnull().any() or (series == 0).any()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if latitude and longitude columns exist\n",
    "            if latitude_column_name in df and longitude_column_name in df:\n",
    "                # Check for NaN values or zeros in both columns\n",
    "                invalid_latitudes = has_invalid_values(df[latitude_column_name])\n",
    "                invalid_longitudes = has_invalid_values(df[longitude_column_name])\n",
    "                \n",
    "                if invalid_latitudes or invalid_longitudes:\n",
    "                    print(f\"File '{file_name}' contains NaN or zero values:\")\n",
    "                    if invalid_latitudes:\n",
    "                        print(f\" - Invalid values in latitude column\")\n",
    "                    if invalid_longitudes:\n",
    "                        print(f\" - Invalid values in longitude column\")\n",
    "                else:\n",
    "                    print(f\"File '{file_name}' has valid latitude and longitude values.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' does not contain the specified latitude and longitude columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9549bdb-b10c-4c7c-a960-ff175051ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your directory\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# Names of the latitude and longitude columns\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "\n",
    "# Function to check for NaN values or zeros\n",
    "def has_invalid_values(series):\n",
    "    return series.isnull().any() or (series == 0).any()\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if latitude and longitude columns exist\n",
    "            if latitude_column_name in df and longitude_column_name in df:\n",
    "                # Check for NaN values or zeros in both columns\n",
    "                invalid_latitudes = has_invalid_values(df[latitude_column_name])\n",
    "                invalid_longitudes = has_invalid_values(df[longitude_column_name])\n",
    "                \n",
    "                if invalid_latitudes or invalid_longitudes:\n",
    "                    # Delete the file\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted '{file_name}' due to invalid latitude or longitude values.\")\n",
    "            else:\n",
    "                print(f\"File '{file_name}' does not contain the specified latitude and longitude columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete. Invalid files have been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de056d0-de1f-4ace-9b18-ee74f96e40fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Replace this with the path to your directory\n",
    "# directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\test'\n",
    "\n",
    "# # Names of the latitude and longitude columns\n",
    "# latitude_column_name = 'latitude'\n",
    "# longitude_column_name = 'longitude'\n",
    "\n",
    "# # Function to remove consecutive duplicates\n",
    "# def remove_consecutive_duplicates(df, column_name):\n",
    "#     # Identify rows where the value in the specified column is equal to that in the next row\n",
    "#     duplicates = df[column_name].eq(df[column_name].shift())\n",
    "#     # Keep rows where there is no such duplication\n",
    "#     return df[~duplicates]\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for file_name in os.listdir(directory_path):\n",
    "#     if file_name.endswith(\".csv\"):\n",
    "#         file_path = os.path.join(directory_path, file_name)\n",
    "#         try:\n",
    "#             df = pd.read_csv(file_path)\n",
    "            \n",
    "#             # Ensure both specified columns exist\n",
    "#             if latitude_column_name in df.columns and longitude_column_name in df.columns:\n",
    "#                 # Remove consecutive duplicates for latitude and longitude\n",
    "#                 df = remove_consecutive_duplicates(df, latitude_column_name)\n",
    "#                 df = remove_consecutive_duplicates(df, longitude_column_name)\n",
    "                \n",
    "#                 # Save the modified DataFrame back to the CSV, overwriting the original\n",
    "#                 df.to_csv(file_path, index=False)\n",
    "#                 print(f\"Processed and updated: {file_name}\")\n",
    "#             else:\n",
    "#                 print(f\"File '{file_name}' does not contain the specified latitude and longitude columns.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# print(\"Processing complete. Consecutive duplicate rows have been removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a5024-c4a9-44d6-94c1-342ba06d4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def infer_date_format(date_str):\n",
    "    try:\n",
    "        parsed_date = parse(date_str, fuzzy=True)\n",
    "        return parsed_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed_remove_duplicate_rows'\n",
    "\n",
    "# Initialize a dictionary to store inferred date formats\n",
    "date_formats = {}\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Attempt to load the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Assuming 'date' is the column name. Adjust as necessary.\n",
    "            if 'date' in df.columns:\n",
    "                # Find the first non-null value in the date column\n",
    "                first_date_str = df['date'].dropna().iloc[0]\n",
    "                # Infer the date format\n",
    "                inferred_format = infer_date_format(first_date_str)\n",
    "                date_formats[file_name] = inferred_format\n",
    "            else:\n",
    "                date_formats[file_name] = 'Date column not found'\n",
    "        except Exception as e:\n",
    "            date_formats[file_name] = f'Error reading file: {e}'\n",
    "\n",
    "# Print the inferred date formats for each file\n",
    "for file_name, date_format in date_formats.items():\n",
    "    print(f\"{file_name}: {date_format}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64c3907-3e2b-40ab-abfa-a93b153109a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hu_Samsung_S22_csv-1706551395849.csv: 1.2313019390581716 seconds\n",
      "Hu_Samsung_S22_csv-1706554333642.csv: 1.8215527230590962 seconds\n",
      "Hu_Samsung_S22_csv-1706556488903.csv: 2.0304373522458627 seconds\n",
      "Hu_Samsung_S22_csv-1706629866070.csv: 14.495664739884393 seconds\n",
      "Hu_Samsung_S22_csv-1706645959366.csv: 15.088645262333594 seconds\n",
      "Hu_Samsung_S22_csv-1706648412755.csv: 13.77317719224871 seconds\n",
      "Hu_Samsung_S22_csv-1706719083524.csv: 19.207185764479203 seconds\n",
      "Hu_Samsung_S22_csv-1707167485257.csv: 0.7369337979094077 seconds\n",
      "Hu_Samsung_S22_csv-1707168946553.csv: 1.4846335697399526 seconds\n",
      "Hu_Samsung_S22_csv-1707169047177.csv: 1.3952180028129395 seconds\n",
      "Hu_Samsung_S22_csv-1707169378518.csv: 4.142857142857143 seconds\n",
      "Hu_Samsung_S22_csv-1707169583100.csv: 1.7659425367904695 seconds\n",
      "Hu_Samsung_S22_csv-1707171684371.csv: 43.113207547169814 seconds\n",
      "Hu_Samsung_S22_csv-1707183508643.csv: 0.8452173913043478 seconds\n",
      "Hu_Samsung_S22_csv-1707228280586.csv: 54.99149453219927 seconds\n",
      "Hu_Samsung_S22_csv-1707229230038.csv: 31.977162629757785 seconds\n",
      "Hu_Samsung_S22_csv-1707229310624.csv: 30.313032089063523 seconds\n",
      "Hu_Samsung_S22_csv-1707234256736.csv: 13.013208026416052 seconds\n",
      "Hu_Samsung_S22_csv-1707235198679.csv: 11.027689706193193 seconds\n",
      "Hu_Samsung_S22_csv-1707255641182.csv: 10.72772935440981 seconds\n",
      "Hu_Samsung_S22_csv-1710938965685.csv: 1.3929446349828516 seconds\n",
      "Hu_Samsung_S22_csv-1710974613701.csv: 10.736680613668062 seconds\n",
      "Hu_Samsung_S22_csv-1711459298652.csv: 1.0186335403726707 seconds\n",
      "Hu_Samsung_S22_csv-1711464703251.csv: 5.1574150787075395 seconds\n",
      "Hu_Samsung_S22_csv-1711464859692.csv: 5.1574150787075395 seconds\n",
      "Hu_Samsung_S22_csv-1711495663127.csv: 13.628724216959512 seconds\n",
      "Hu_Samsung_S22_csv-1711503190470.csv: 1.0344827586206897 seconds\n",
      "Hu_Samsung_S22_csv-1711503261620.csv: 2.8055555555555554 seconds\n",
      "Hu_Samsung_S22_csv-1711561058750.csv: 0.7910014513788098 seconds\n",
      "Hu_Samsung_S22_csv-1711561852741.csv: 2.652173913043478 seconds\n",
      "Hu_Samsung_S22_csv-1711579393686.csv: 19.450828729281767 seconds\n",
      "Hu_Samsung_S22_csv-1711580753418.csv: 10.786689419795222 seconds\n",
      "Hu_Samsung_S22_csv-1711590532830.csv: 12.573053368328958 seconds\n",
      "Hu_Samsung_S22_csv-1711590978447.csv: 12.520600858369098 seconds\n",
      "Hu_Samsung_S22_csv-1711591136725.csv: 12.071575483340188 seconds\n",
      "Hu_Samsung_S22_csv-1711591342117.csv: 11.248953178530643 seconds\n",
      "Hu_Samsung_S22_csv-1711591563618.csv: 10.524213503004596 seconds\n",
      "Hu_Samsung_S22_csv-1711591794067.csv: 9.60064 seconds\n",
      "Hu_Samsung_S22_csv-1711592146866.csv: 8.553395322626091 seconds\n",
      "Hu_Samsung_S22_csv-1711625495779.csv: 1.1 seconds\n",
      "Hu_Samsung_S22_csv-1711625506170.csv: 1.9090909090909092 seconds\n",
      "Hu_Samsung_S22_csv-1711625524337.csv: 1.8181818181818181 seconds\n",
      "Hu_Samsung_S22_csv-1711625595119.csv: 2.4878048780487805 seconds\n",
      "Hu_Samsung_S22_csv-1711626949733.csv: 1.0941000746825988 seconds\n",
      "Hu_Samsung_S22_csv-1711632741135.csv: 4.194219653179191 seconds\n",
      "Hu_Samsung_S22_csv-1711635308022.csv: 4.838423645320197 seconds\n",
      "Li_New_Pixel_6_csv-1668118675414.csv: 2.734848484848485 seconds\n",
      "Li_New_Pixel_6_csv-1668641647615.csv: 31.8 seconds\n",
      "Li_New_Pixel_6_csv-1668642791074.csv: 54.095238095238095 seconds\n",
      "Li_New_Pixel_6_csv-1668643855169.csv: 55.8235294117647 seconds\n",
      "Li_New_Pixel_6_csv-1668663056927.csv: 4.0 seconds\n",
      "Li_New_Pixel_6_csv-1670970167802.csv: 4.938610662358643 seconds\n",
      "Li_New_Pixel_6_csv-1670998572766.csv: 4.101149425287356 seconds\n",
      "Li_New_Pixel_6_csv-1671061797748.csv: 5.298013245033113 seconds\n",
      "Li_New_Pixel_6_csv-1671083160556.csv: 4.580357142857143 seconds\n",
      "Li_New_Pixel_7_csv-1670970167265.csv: 4.938610662358643 seconds\n",
      "Li_New_Pixel_7_csv-1670998574545.csv: 4.101149425287356 seconds\n",
      "Li_New_Pixel_7_csv-1671061783309.csv: 4.716666666666667 seconds\n",
      "Li_New_Pixel_7_csv-1671083148882.csv: 4.303703703703704 seconds\n",
      "Li_OnePlus_10Pro_csv-1673457344394.csv: 2.92 seconds\n",
      "Li_OnePlus_10Pro_csv-1673479648498.csv: 2.5988538681948423 seconds\n",
      "Li_OnePlus_10Pro_csv-1673481264831.csv: 2.314814814814815 seconds\n",
      "Li_OnePlus_10Pro_csv-1673482374494.csv: 2.9076923076923076 seconds\n",
      "Li_OnePlus_10Pro_csv-1673483015181.csv: 4.129032258064516 seconds\n",
      "Li_OnePlus_10Pro_csv-1673483696088.csv: 2.4075829383886256 seconds\n",
      "Li_OnePlus_10Pro_csv-1673496675759.csv: 2.608695652173913 seconds\n",
      "Li_OnePlus_10Pro_csv-1673553885461.csv: 5.0 seconds\n",
      "Li_OnePlus_10Pro_csv-1673645167198.csv: 4.483516483516484 seconds\n",
      "Li_OnePlus_10Pro_csv-1673647172071.csv: 4.972972972972973 seconds\n",
      "Li_OnePlus_10Pro_csv-1673647928510.csv: 5.669291338582677 seconds\n",
      "Li_OnePlus_10Pro_csv-1673649592614.csv: 4.775147928994083 seconds\n",
      "Li_OnePlus_10Pro_csv-1673675238354.csv: 2.9381443298969074 seconds\n",
      "Li_OnePlus_10Pro_csv-1673675670474.csv: 3.787037037037037 seconds\n",
      "Li_OnePlus_10Pro_csv-1673676046708.csv: 3.9887640449438204 seconds\n",
      "Li_OnePlus_10Pro_csv-1673734528629.csv: 5.053191489361702 seconds\n",
      "Li_OnePlus_10Pro_csv-1673739906247.csv: 4.508670520231214 seconds\n",
      "Li_OnePlus_10Pro_csv-1673742208228.csv: 3.711864406779661 seconds\n",
      "Li_OnePlus_10Pro_csv-1673971107670.csv: 4.511578947368421 seconds\n",
      "Li_OnePlus_10Pro_csv-1674162989922.csv: 5.03125 seconds\n",
      "Li_OnePlus_10Pro_csv-1674328160260.csv: 4.956896551724138 seconds\n",
      "Li_OnePlus_10Pro_csv-1674334303748.csv: 5.041095890410959 seconds\n",
      "Li_OnePlus_10Pro_csv-1674576444099.csv: 4.418563922942207 seconds\n",
      "Li_OnePlus_10Pro_csv-1674851280487.csv: 4.778425655976676 seconds\n",
      "Li_OnePlus_10Pro_csv-1674873486201.csv: 4.857142857142857 seconds\n",
      "Li_OnePlus_10Pro_csv-1674877773799.csv: 4.953125 seconds\n",
      "Li_OnePlus_10Pro_csv-1674882148277.csv: 4.925 seconds\n",
      "Li_OnePlus_10Pro_csv-1675017242228.csv: 5.006896551724138 seconds\n",
      "Li_OnePlus_10Pro_csv-1675020893147.csv: 4.982954545454546 seconds\n",
      "Li_OnePlus_10Pro_csv-1675023955938.csv: 5.03030303030303 seconds\n",
      "Li_OnePlus_10Pro_csv-1675181760885.csv: 4.403141361256544 seconds\n",
      "Li_OnePlus_10Pro_csv-1675195708164.csv: 5.0 seconds\n",
      "Li_OnePlus_10Pro_csv-1675200446354.csv: 4.978354978354979 seconds\n",
      "Li_OnePlus_10Pro_csv-1675883373286.csv: 4.323045267489712 seconds\n",
      "Li_OnePlus_10Pro_csv-1675901932423.csv: 5.1144544431946 seconds\n",
      "Li_Pixel_6_csv-1666143336657.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143905551.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143925579.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143930889.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666144178897.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666283977858.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666284481675.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666385250356.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666717979729.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666718201515.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666844900709.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666845069838.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666902743826.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666903066604.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666910588148.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666911178839.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666911326377.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666911552038.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666912251226.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666913635272.csv: 5.0 seconds\n",
      "Li_Pixel_6_csv-1668101143832.csv: 3.663157894736842 seconds\n",
      "Li_Samsung_S10_csv-1672872507940.csv: 2.7058823529411766 seconds\n",
      "Li_Samsung_S10_csv-1673044775501.csv: 2.6666666666666665 seconds\n",
      "Li_Samsung_S10_csv-1673044903212.csv: 2.1666666666666665 seconds\n",
      "Li_Samsung_S10_csv-1674091794910.csv: 4.199460916442049 seconds\n",
      "Li_Samsung_S10_csv-1674093009300.csv: 3.7282051282051283 seconds\n",
      "Li_Samsung_S10_csv-1674507931440.csv: 4.261875761266748 seconds\n",
      "Li_Samsung_S10_csv-1674586132384.csv: 2.640378548895899 seconds\n",
      "Li_Samsung_S10_csv-1674593675251.csv: 3.339171974522293 seconds\n",
      "Li_Samsung_S10_csv-1674595237096.csv: 2.914230019493177 seconds\n",
      "Li_Samsung_S21_csv-1669614843214.csv: 6.55 seconds\n",
      "Li_Samsung_S21_csv-1671507198416.csv: 3.727272727272727 seconds\n",
      "Li_Samsung_S21_csv-1671515190653.csv: 4.80897583429229 seconds\n",
      "Li_Samsung_S21_csv-1673744211840.csv: 4.384937238493724 seconds\n",
      "Li_Samsung_S21_csv-1673887278434.csv: 77.83234042553191 seconds\n",
      "Li_Samsung_S21_csv-1674120305812.csv: 6.367306155075939 seconds\n",
      "Li_Samsung_S21_csv-1674201551799.csv: 48.623313545601725 seconds\n",
      "\n",
      "Overall average sampling interval across all files: 8.207019569154292 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def correct_24_hour_time(date_str):\n",
    "    # Check if the time starts with \"24\"\n",
    "    if date_str[-8:-6] == \"24\":\n",
    "        # Replace \"24\" with \"00\"\n",
    "        corrected_str = date_str[:-8] + \"00\" + date_str[-6:]\n",
    "        # Convert to datetime\n",
    "        corrected_datetime = pd.to_datetime(corrected_str)\n",
    "        # Add a day to the date\n",
    "        corrected_datetime += timedelta(days=1)\n",
    "        return corrected_datetime\n",
    "    else:\n",
    "        # If there's no \"24\" hour, just convert to datetime\n",
    "        return pd.to_datetime(date_str)\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed_remove_duplicate_rows_without_flights'\n",
    "\n",
    "# Initialize a list to store the results\n",
    "average_intervals = []\n",
    "\n",
    "# Sum of all average intervals\n",
    "total_average_interval = 0\n",
    "\n",
    "# Count of files processed for the overall average calculation\n",
    "file_count = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            # Apply the correction for the \"24:00:XX\" case and convert to datetime\n",
    "            df['date'] = df['date'].apply(correct_24_hour_time)\n",
    "            \n",
    "            # Ensure the data is sorted by 'date'\n",
    "            df = df.sort_values(by='date')\n",
    "            \n",
    "            # Calculate differences (intervals) between each timestamp\n",
    "            df['sampling_interval_seconds'] = df['date'].diff().dt.total_seconds()\n",
    "            \n",
    "            # Ignore NaN values for the average calculation\n",
    "            valid_intervals = df['sampling_interval_seconds'].dropna()\n",
    "            \n",
    "            if not valid_intervals.empty:\n",
    "                average_sampling_interval = valid_intervals.mean()\n",
    "                average_intervals.append((file_name, average_sampling_interval))\n",
    "                total_average_interval += average_sampling_interval\n",
    "                file_count += 1\n",
    "            else:\n",
    "                average_intervals.append((file_name, 'Insufficient data for interval calculation'))\n",
    "\n",
    "overall_average_interval = total_average_interval / file_count if file_count > 0 else 'No valid data found in any file'\n",
    "\n",
    "for file_name, interval in average_intervals:\n",
    "    print(f\"{file_name}: {interval} seconds\")\n",
    "\n",
    "print(f\"\\nOverall average sampling interval across all files: {overall_average_interval} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ddaaa33-ac3c-4ed0-8ff9-9254fd3ab43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hu_Samsung_S22_csv-1706551395849.csv: 1.2313019390581716 seconds\n",
      "Hu_Samsung_S22_csv-1706554333642.csv: 1.8215527230590962 seconds\n",
      "Hu_Samsung_S22_csv-1706556488903.csv: 2.0304373522458627 seconds\n",
      "Hu_Samsung_S22_csv-1706629866070.csv: 14.495664739884393 seconds\n",
      "Hu_Samsung_S22_csv-1706645959366.csv: 15.088645262333594 seconds\n",
      "Hu_Samsung_S22_csv-1706648412755.csv: 13.77317719224871 seconds\n",
      "Hu_Samsung_S22_csv-1706719083524.csv: 19.207185764479203 seconds\n",
      "Hu_Samsung_S22_csv-1707167485257.csv: 0.7369337979094077 seconds\n",
      "Hu_Samsung_S22_csv-1707168946553.csv: 1.4846335697399526 seconds\n",
      "Hu_Samsung_S22_csv-1707169047177.csv: 1.3952180028129395 seconds\n",
      "Hu_Samsung_S22_csv-1707169378518.csv: 4.142857142857143 seconds\n",
      "Hu_Samsung_S22_csv-1707169583100.csv: 1.7659425367904695 seconds\n",
      "Hu_Samsung_S22_csv-1707171684371.csv: 43.113207547169814 seconds\n",
      "Hu_Samsung_S22_csv-1707183508643.csv: 0.8452173913043478 seconds\n",
      "Hu_Samsung_S22_csv-1707228280586.csv: 54.99149453219927 seconds\n",
      "Hu_Samsung_S22_csv-1707229230038.csv: 31.977162629757785 seconds\n",
      "Hu_Samsung_S22_csv-1707229310624.csv: 30.313032089063523 seconds\n",
      "Hu_Samsung_S22_csv-1707234256736.csv: 13.013208026416052 seconds\n",
      "Hu_Samsung_S22_csv-1707235198679.csv: 11.027689706193193 seconds\n",
      "Hu_Samsung_S22_csv-1707255641182.csv: 10.72772935440981 seconds\n",
      "Hu_Samsung_S22_csv-1710938965685.csv: 1.3929446349828516 seconds\n",
      "Hu_Samsung_S22_csv-1710974613701.csv: 10.736680613668062 seconds\n",
      "Hu_Samsung_S22_csv-1711459298652.csv: 1.0186335403726707 seconds\n",
      "Hu_Samsung_S22_csv-1711464703251.csv: 5.1574150787075395 seconds\n",
      "Hu_Samsung_S22_csv-1711464859692.csv: 5.1574150787075395 seconds\n",
      "Hu_Samsung_S22_csv-1711495663127.csv: 13.628724216959512 seconds\n",
      "Hu_Samsung_S22_csv-1711503190470.csv: 1.0344827586206897 seconds\n",
      "Hu_Samsung_S22_csv-1711503261620.csv: 2.8055555555555554 seconds\n",
      "Hu_Samsung_S22_csv-1711561058750.csv: 0.7910014513788098 seconds\n",
      "Hu_Samsung_S22_csv-1711561852741.csv: 2.652173913043478 seconds\n",
      "Hu_Samsung_S22_csv-1711579393686.csv: 19.450828729281767 seconds\n",
      "Hu_Samsung_S22_csv-1711580753418.csv: 10.786689419795222 seconds\n",
      "Hu_Samsung_S22_csv-1711590532830.csv: 12.573053368328958 seconds\n",
      "Hu_Samsung_S22_csv-1711590978447.csv: 12.520600858369098 seconds\n",
      "Hu_Samsung_S22_csv-1711591136725.csv: 12.071575483340188 seconds\n",
      "Hu_Samsung_S22_csv-1711591342117.csv: 11.248953178530643 seconds\n",
      "Hu_Samsung_S22_csv-1711591563618.csv: 10.524213503004596 seconds\n",
      "Hu_Samsung_S22_csv-1711591794067.csv: 9.60064 seconds\n",
      "Hu_Samsung_S22_csv-1711592146866.csv: 8.553395322626091 seconds\n",
      "Hu_Samsung_S22_csv-1711625495779.csv: 1.1 seconds\n",
      "Hu_Samsung_S22_csv-1711625506170.csv: 1.9090909090909092 seconds\n",
      "Hu_Samsung_S22_csv-1711625524337.csv: 1.8181818181818181 seconds\n",
      "Hu_Samsung_S22_csv-1711625595119.csv: 2.4878048780487805 seconds\n",
      "Hu_Samsung_S22_csv-1711626949733.csv: 1.0941000746825988 seconds\n",
      "Hu_Samsung_S22_csv-1711632741135.csv: 4.194219653179191 seconds\n",
      "Hu_Samsung_S22_csv-1711635308022.csv: 4.838423645320197 seconds\n",
      "Li_New_Pixel_6_csv-1668118675414.csv: 2.734848484848485 seconds\n",
      "Li_New_Pixel_6_csv-1668641647615.csv: 31.8 seconds\n",
      "Li_New_Pixel_6_csv-1668642791074.csv: 54.095238095238095 seconds\n",
      "Li_New_Pixel_6_csv-1668643855169.csv: 55.8235294117647 seconds\n",
      "Li_New_Pixel_6_csv-1668663056927.csv: 4.0 seconds\n",
      "Li_New_Pixel_6_csv-1670970167802.csv: 4.938610662358643 seconds\n",
      "Li_New_Pixel_6_csv-1670998572766.csv: 4.101149425287356 seconds\n",
      "Li_New_Pixel_6_csv-1671061797748.csv: 5.298013245033113 seconds\n",
      "Li_New_Pixel_6_csv-1671083160556.csv: 4.580357142857143 seconds\n",
      "Li_New_Pixel_7_csv-1670970167265.csv: 4.938610662358643 seconds\n",
      "Li_New_Pixel_7_csv-1670998574545.csv: 4.101149425287356 seconds\n",
      "Li_New_Pixel_7_csv-1671061783309.csv: 4.716666666666667 seconds\n",
      "Li_New_Pixel_7_csv-1671083148882.csv: 4.303703703703704 seconds\n",
      "Li_OnePlus_10Pro_csv-1673457344394.csv: 2.92 seconds\n",
      "Li_OnePlus_10Pro_csv-1673479648498.csv: 2.5988538681948423 seconds\n",
      "Li_OnePlus_10Pro_csv-1673481264831.csv: 2.314814814814815 seconds\n",
      "Li_OnePlus_10Pro_csv-1673482374494.csv: 2.9076923076923076 seconds\n",
      "Li_OnePlus_10Pro_csv-1673483015181.csv: 4.129032258064516 seconds\n",
      "Li_OnePlus_10Pro_csv-1673483696088.csv: 2.4075829383886256 seconds\n",
      "Li_OnePlus_10Pro_csv-1673496675759.csv: 2.608695652173913 seconds\n",
      "Li_OnePlus_10Pro_csv-1673553885461.csv: 5.0 seconds\n",
      "Li_OnePlus_10Pro_csv-1673645167198.csv: 4.483516483516484 seconds\n",
      "Li_OnePlus_10Pro_csv-1673647172071.csv: 4.972972972972973 seconds\n",
      "Li_OnePlus_10Pro_csv-1673647928510.csv: 5.669291338582677 seconds\n",
      "Li_OnePlus_10Pro_csv-1673649592614.csv: 4.775147928994083 seconds\n",
      "Li_OnePlus_10Pro_csv-1673675238354.csv: 2.9381443298969074 seconds\n",
      "Li_OnePlus_10Pro_csv-1673675670474.csv: 3.787037037037037 seconds\n",
      "Li_OnePlus_10Pro_csv-1673676046708.csv: 3.9887640449438204 seconds\n",
      "Li_OnePlus_10Pro_csv-1673734528629.csv: 5.053191489361702 seconds\n",
      "Li_OnePlus_10Pro_csv-1673739906247.csv: 4.508670520231214 seconds\n",
      "Li_OnePlus_10Pro_csv-1673742208228.csv: 3.711864406779661 seconds\n",
      "Li_OnePlus_10Pro_csv-1673971107670.csv: 4.511578947368421 seconds\n",
      "Li_OnePlus_10Pro_csv-1674162989922.csv: 5.03125 seconds\n",
      "Li_OnePlus_10Pro_csv-1674328160260.csv: 4.956896551724138 seconds\n",
      "Li_OnePlus_10Pro_csv-1674334303748.csv: 5.041095890410959 seconds\n",
      "Li_OnePlus_10Pro_csv-1674576444099.csv: 4.418563922942207 seconds\n",
      "Li_OnePlus_10Pro_csv-1674851280487.csv: 4.778425655976676 seconds\n",
      "Li_OnePlus_10Pro_csv-1674873486201.csv: 4.857142857142857 seconds\n",
      "Li_OnePlus_10Pro_csv-1674877773799.csv: 4.953125 seconds\n",
      "Li_OnePlus_10Pro_csv-1674882148277.csv: 4.925 seconds\n",
      "Li_OnePlus_10Pro_csv-1675017242228.csv: 5.006896551724138 seconds\n",
      "Li_OnePlus_10Pro_csv-1675020893147.csv: 4.982954545454546 seconds\n",
      "Li_OnePlus_10Pro_csv-1675023955938.csv: 5.03030303030303 seconds\n",
      "Li_OnePlus_10Pro_csv-1675181760885.csv: 4.403141361256544 seconds\n",
      "Li_OnePlus_10Pro_csv-1675195708164.csv: 5.0 seconds\n",
      "Li_OnePlus_10Pro_csv-1675200446354.csv: 4.978354978354979 seconds\n",
      "Li_OnePlus_10Pro_csv-1675883373286.csv: 4.323045267489712 seconds\n",
      "Li_OnePlus_10Pro_csv-1675901932423.csv: 5.1144544431946 seconds\n",
      "Li_Pixel_6_csv-1666143336657.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143905551.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143925579.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666143930889.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666144178897.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666283977858.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666284481675.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666385250356.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666717979729.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666718201515.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666844900709.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666845069838.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666902743826.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666903066604.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666910588148.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666911178839.csv: Insufficient data for interval calculation seconds\n",
      "Li_Pixel_6_csv-1666911326377.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666911552038.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666912251226.csv: 0.0 seconds\n",
      "Li_Pixel_6_csv-1666913635272.csv: 5.0 seconds\n",
      "Li_Pixel_6_csv-1668101143832.csv: 3.663157894736842 seconds\n",
      "Li_Samsung_S10_csv-1672872507940.csv: 2.7058823529411766 seconds\n",
      "Li_Samsung_S10_csv-1673044775501.csv: 2.6666666666666665 seconds\n",
      "Li_Samsung_S10_csv-1673044903212.csv: 2.1666666666666665 seconds\n",
      "Li_Samsung_S10_csv-1674091794910.csv: 4.199460916442049 seconds\n",
      "Li_Samsung_S10_csv-1674093009300.csv: 3.7282051282051283 seconds\n",
      "Li_Samsung_S10_csv-1674507931440.csv: 4.261875761266748 seconds\n",
      "Li_Samsung_S10_csv-1674586132384.csv: 2.640378548895899 seconds\n",
      "Li_Samsung_S10_csv-1674593675251.csv: 3.339171974522293 seconds\n",
      "Li_Samsung_S10_csv-1674595237096.csv: 2.914230019493177 seconds\n",
      "Li_Samsung_S21_csv-1669614843214.csv: 6.55 seconds\n",
      "Li_Samsung_S21_csv-1669675106111_flight.csv: 5.786407766990291 seconds\n",
      "Li_Samsung_S21_csv-1669676627475_flight.csv: 5.96 seconds\n",
      "Li_Samsung_S21_csv-1671507198416.csv: 3.727272727272727 seconds\n",
      "Li_Samsung_S21_csv-1671515190653.csv: 4.80897583429229 seconds\n",
      "Li_Samsung_S21_csv-1672726168810_flight.csv: 584.3825503355705 seconds\n",
      "Li_Samsung_S21_csv-1672736082052_flight.csv: 65.57142857142857 seconds\n",
      "Li_Samsung_S21_csv-1672739687207_flight.csv: 6.859259259259259 seconds\n",
      "Li_Samsung_S21_csv-1673744211840.csv: 4.384937238493724 seconds\n",
      "Li_Samsung_S21_csv-1673785930210_flight.csv: 13.296296296296296 seconds\n",
      "Li_Samsung_S21_csv-1673792926809_flight.csv: Insufficient data for interval calculation seconds\n",
      "Li_Samsung_S21_csv-1673820090376_flight.csv: Insufficient data for interval calculation seconds\n",
      "Li_Samsung_S21_csv-1673887278434.csv: 77.83234042553191 seconds\n",
      "Li_Samsung_S21_csv-1674120305812.csv: 6.367306155075939 seconds\n",
      "Li_Samsung_S21_csv-1674156084407_flight.csv: 185.19444444444446 seconds\n",
      "Li_Samsung_S21_csv-1674201551799.csv: 48.623313545601725 seconds\n",
      "\n",
      "Overall average sampling interval across all files: 14.683829566673568 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def correct_24_hour_time(date_str):\n",
    "    # Check if the time starts with \"24\"\n",
    "    if date_str[-8:-6] == \"24\":\n",
    "        # Replace \"24\" with \"00\"\n",
    "        corrected_str = date_str[:-8] + \"00\" + date_str[-6:]\n",
    "        # Convert to datetime\n",
    "        corrected_datetime = pd.to_datetime(corrected_str)\n",
    "        # Add a day to the date\n",
    "        corrected_datetime += timedelta(days=1)\n",
    "        return corrected_datetime\n",
    "    else:\n",
    "        # If there's no \"24\" hour, just convert to datetime\n",
    "        return pd.to_datetime(date_str)\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "directory_path = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed_remove_duplicate_rows'\n",
    "\n",
    "# Initialize a list to store the results\n",
    "average_intervals = []\n",
    "\n",
    "# Sum of all average intervals\n",
    "total_average_interval = 0\n",
    "\n",
    "# Count of files processed for the overall average calculation\n",
    "file_count = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            # Apply the correction for the \"24:00:XX\" case and convert to datetime\n",
    "            df['date'] = df['date'].apply(correct_24_hour_time)\n",
    "            \n",
    "            # Ensure the data is sorted by 'date'\n",
    "            df = df.sort_values(by='date')\n",
    "            \n",
    "            # Calculate differences (intervals) between each timestamp\n",
    "            df['sampling_interval_seconds'] = df['date'].diff().dt.total_seconds()\n",
    "            \n",
    "            # Ignore NaN values for the average calculation\n",
    "            valid_intervals = df['sampling_interval_seconds'].dropna()\n",
    "            \n",
    "            if not valid_intervals.empty:\n",
    "                average_sampling_interval = valid_intervals.mean()\n",
    "                average_intervals.append((file_name, average_sampling_interval))\n",
    "                total_average_interval += average_sampling_interval\n",
    "                file_count += 1\n",
    "            else:\n",
    "                average_intervals.append((file_name, 'Insufficient data for interval calculation'))\n",
    "\n",
    "overall_average_interval = total_average_interval / file_count if file_count > 0 else 'No valid data found in any file'\n",
    "\n",
    "for file_name, interval in average_intervals:\n",
    "    print(f\"{file_name}: {interval} seconds\")\n",
    "\n",
    "print(f\"\\nOverall average sampling interval across all files: {overall_average_interval} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe788bb4-01c3-4c9d-8e82-82acc592c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Distance Traversed: 8473.671461837745 km\n",
      "Average Distance: 66.48728471092323 m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def calculate_distance_for_file(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Calculate the distances\n",
    "    distances = [\n",
    "        haversine(lon1, lat1, lon2, lat2) \n",
    "        for lat1, lon1, lat2, lon2 in zip(df['latitude'][:-1], df['longitude'][:-1], df['latitude'][1:], df['longitude'][1:])\n",
    "    ]\n",
    "    \n",
    "    # Sum the distances\n",
    "    return sum(distances), len(df)\n",
    "\n",
    "def calculate_cumulative_distance_and_row_count(directory):\n",
    "    \n",
    "    total_distance = 0\n",
    "    total_row_count = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            distance, row_count = calculate_distance_for_file(file_path)\n",
    "            total_distance += distance\n",
    "            total_row_count += row_count\n",
    "\n",
    "    average_distance =  (total_distance/ total_row_count)*1000\n",
    "    return total_distance, average_distance\n",
    "\n",
    "# Example usage\n",
    "directory = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed_remove_duplicate_rows'  # Replace this with the path to your directory\n",
    "cumulative_distance, average_distance = calculate_cumulative_distance_and_row_count(directory)\n",
    "print(f\"Cumulative Distance Traversed: {cumulative_distance} km\")\n",
    "print(f\"Average Distance: {average_distance} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe3bce-50be-4de6-874a-0f6a3544d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def calculate_distance_for_file(csv_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Calculate the distances\n",
    "    distances = [\n",
    "        haversine(lon1, lat1, lon2, lat2) \n",
    "        for lat1, lon1, lat2, lon2 in zip(df['latitude'][:-1], df['longitude'][:-1], df['latitude'][1:], df['longitude'][1:])\n",
    "    ]\n",
    "    \n",
    "    # Sum the distances\n",
    "    return sum(distances)\n",
    "\n",
    "def calculate_cumulative_distance(directory):\n",
    "    total_distance = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            total_distance += calculate_distance_for_file(file_path)\n",
    "    return total_distance\n",
    "\n",
    "# Example usage\n",
    "directory = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed_remove_duplicate_rows'  # Replace this with the path to your directory\n",
    "cumulative_distance = calculate_cumulative_distance(directory)\n",
    "print(f\"Cumulative Distance Traversed: {cumulative_distance} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1b81b-a053-476e-bd67-8452c31f1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\Datasets\\\\Collected\\\\dataset_preprocessed'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an identifier starting from 1\n",
    "identifier = 1\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for file_name in csv_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Add a new column 'identifier' with all rows having the same identifier\n",
    "    df['identifier'] = identifier\n",
    "    \n",
    "    # Save the modified DataFrame back to the same CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Increment the identifier for the next file\n",
    "    identifier += 1\n",
    "\n",
    "print('All files have been processed and updated with an identifier.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582cd0c-30ee-4ddb-a905-cba295cb4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Data Security ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7c0ce2-74ac-47d2-a6fd-3433e7cf5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved to C:\\Users\\ss6365\\Desktop\\location_privacy_final\\collected\\data\\merged_all_utility_subset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Input directory containing CSV files\n",
    "input_directory = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\location_privacy_final\\\\collected\\\\data\\\\utility'\n",
    "\n",
    "# Output directory where the merged CSV file will be saved\n",
    "output_directory = 'C:\\\\Users\\\\ss6365\\\\Desktop\\\\location_privacy_final\\\\collected\\\\data'\n",
    "\n",
    "\n",
    "\n",
    "# # List of important columns to keep\n",
    "important_columns = ['identifier', 'longitude', 'latitude']  # Replace with your column names\n",
    "\n",
    "# # Create a list to store dataframes from individual CSV files\n",
    "dataframes = []\n",
    "\n",
    "# # Iterate through CSV files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        df = pd.read_csv(file_path, usecols=important_columns)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# # Concatenate dataframes vertically (along rows)\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# # Output file path for the merged CSV file\n",
    "output_file_path = os.path.join(output_directory, 'merged_all_utility_subset.csv')\n",
    "\n",
    "# # Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged CSV file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e274d172-e3c9-4e68-88c7-836b500a4115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-77.680333</td>\n",
       "      <td>43.083838</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-77.680991</td>\n",
       "      <td>43.083803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-77.681017</td>\n",
       "      <td>43.083802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-77.681042</td>\n",
       "      <td>43.083802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-77.681090</td>\n",
       "      <td>43.083802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99218</th>\n",
       "      <td>-77.680445</td>\n",
       "      <td>43.083879</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99219</th>\n",
       "      <td>-77.680442</td>\n",
       "      <td>43.083868</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99220</th>\n",
       "      <td>-77.680441</td>\n",
       "      <td>43.083863</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99221</th>\n",
       "      <td>-77.680442</td>\n",
       "      <td>43.083860</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99222</th>\n",
       "      <td>-77.680442</td>\n",
       "      <td>43.083850</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55425 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude   latitude  identifier\n",
       "0     -77.680333  43.083838           1\n",
       "1     -77.680991  43.083803           1\n",
       "2     -77.681017  43.083802           1\n",
       "3     -77.681042  43.083802           1\n",
       "4     -77.681090  43.083802           1\n",
       "...          ...        ...         ...\n",
       "99218 -77.680445  43.083879          46\n",
       "99219 -77.680442  43.083868          46\n",
       "99220 -77.680441  43.083863          46\n",
       "99221 -77.680442  43.083860          46\n",
       "99222 -77.680442  43.083850          46\n",
       "\n",
       "[55425 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\ss6365\\Desktop\\location_privacy_final\\collected\\data\\merged_all_utility_subset.csv')\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in km\n",
    "    R = 6371.0\n",
    "    # Convert coordinates from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def find_square_boundaries(lat, lon, distance_km):\n",
    "    # Approximate conversions\n",
    "    delta_lat = distance_km / 111  # 111 km per degree of latitude\n",
    "    delta_lon = distance_km / (111 * np.cos(np.radians(lat)))  # Adjust for longitude\n",
    "    return lat - delta_lat, lat + delta_lat, lon - delta_lon, lon + delta_lon\n",
    "\n",
    "\n",
    "# Calculate the median (or mean) latitude and longitude\n",
    "central_lat = df['latitude'].median()\n",
    "central_lon = df['longitude'].median()\n",
    "\n",
    "\n",
    "# Define the square region boundaries\n",
    "lat_min, lat_max, lon_min, lon_max = find_square_boundaries(central_lat, central_lon, 2)\n",
    "\n",
    "# Filter the DataFrame for points within the 1 km square\n",
    "df_limit = df[(df['latitude'] >= lat_min) & (df['latitude'] <= lat_max) &\n",
    "               (df['longitude'] >= lon_min) & (df['longitude'] <= lon_max)]\n",
    "\n",
    "df_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e2eb8df-a579-4e6d-8789-e046d3421aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = r'C:\\Users\\ss6365\\Desktop\\location_privacy_final\\collected\\data\\utility'\n",
    "output_directory = r'C:\\Users\\ss6365\\Desktop\\location_privacy_final\\collected\\data\\security'\n",
    "\n",
    "\n",
    "# Calculate the boundaries based on the current file\n",
    "lat_min = df_limit['latitude'].min()\n",
    "lat_max = df_limit['latitude'].max()\n",
    "lon_min = df_limit['longitude'].min()\n",
    "lon_max = df_limit['longitude'].max()\n",
    "\n",
    "\n",
    "# Iterate through CSV files in the input directory\n",
    "for csv_file in glob.glob(os.path.join(input_directory, '*.csv')):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "\n",
    "    # Distance parameter (can be adjusted as needed)\n",
    "    distance_km = 2\n",
    "\n",
    "    # Define the square region boundaries and filter the DataFrame\n",
    "    df_square = df[(df['latitude'] >= lat_min) & (df['latitude'] <= lat_max) &\n",
    "                   (df['longitude'] >= lon_min) & (df['longitude'] <= lon_max)]\n",
    "\n",
    "    # Check if the filtered DataFrame is empty (no data within boundaries)\n",
    "    if df_square.empty:\n",
    "        continue  # Skip saving if no data matches the criteria\n",
    "\n",
    "    # Extract the base filename without extension\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    \n",
    "    # Create the new filename with distance_km\n",
    "    new_filename = f\"{base_filename}_{distance_km}km.csv\"\n",
    "    \n",
    "    # Save the filtered DataFrame to the output directory with the new filename\n",
    "    output_path = os.path.join(output_directory, new_filename)\n",
    "    df_square.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6581c-e08e-4595-b733-1d6c1db82b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
